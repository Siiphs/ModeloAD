{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyDataverse.api import NativeApi, DataAccessApi\n",
    "\n",
    "# CONFIGURACIÓN \n",
    "API_KEY = \"7386f7e6-18cc-49ee-a8eb-d08ac888783d\"\n",
    "BASE_URL = \"https://datospararesiliencia.cl\"\n",
    "\n",
    "DATA_DIR = \"datos_eventos\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "DOIS = [\n",
    "    \"doi:10.71578/FMGXND\",\n",
    "    \"doi:10.71578/UXAUN5\",\n",
    "    \"doi:10.71578/QRDY49\",\n",
    "    \"doi:10.71578/HKLGZ8\",\n",
    "    \"doi:10.71578/TLSNSV\",\n",
    "]\n",
    "\n",
    "VALID_EXT = {\".csv\", \".xlsx\", \".xls\"}\n",
    "\n",
    "\n",
    "api = NativeApi(BASE_URL, API_KEY)\n",
    "data_api = DataAccessApi(BASE_URL, API_KEY)\n",
    "\n",
    "# UTILIDADES\n",
    "# Corregir caracteres maldecodificados \n",
    "def fix_encoding(col):\n",
    "    try:\n",
    "        return col.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return col\n",
    "\n",
    "\n",
    "# Detectar columna Región\n",
    "def detectar_columna_region(df):\n",
    "    for col in df.columns:\n",
    "        c = col.lower().strip()\n",
    "        if \"regi\" in c:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# Detectar columna Fecha\n",
    "def detectar_columna_fecha(df):\n",
    "    posibles = [\"fecha\", \"fecha_update\"]\n",
    "    for col in df.columns:\n",
    "        if col.lower().strip() in posibles:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# Normalizar nombre de región final\n",
    "def normalizar_region(valor):\n",
    "    v = str(valor).lower()\n",
    "    if \"bio\" in v:\n",
    "        return \"BioBio\"\n",
    "    if \"nuble\" in v:\n",
    "        return \"Ñuble\"\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# API descargar archivos\n",
    "def download_dataset_files(doi):\n",
    "    ds = api.get_dataset(doi)\n",
    "\n",
    "    if ds.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    files = ds.json()[\"data\"][\"latestVersion\"][\"files\"]\n",
    "    downloaded_paths = []\n",
    "\n",
    "    for f in files:\n",
    "        fname = f[\"dataFile\"][\"filename\"]\n",
    "        fid = f[\"dataFile\"][\"id\"]\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "\n",
    "        #print(f\" Archivo: {fname}, id={fid}\")\n",
    "\n",
    "        if ext not in VALID_EXT:\n",
    "            continue\n",
    "\n",
    "        resp = data_api.get_datafile(fid, is_pid=False)\n",
    "        if resp.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(DATA_DIR, fname)\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "\n",
    "        downloaded_paths.append(path)\n",
    "\n",
    "    return downloaded_paths\n",
    "\n",
    "# PROCESAR Y LIMPIAR TABLAS\n",
    "def load_and_clean_file(path):\n",
    "    try:\n",
    "        # Cargar archivo\n",
    "        if path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "        elif path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "            df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # Corregir encoding de columnas\n",
    "        df.columns = [fix_encoding(c) for c in df.columns]\n",
    "\n",
    "        # Separar columnas fusionadas con \"|\"\n",
    "        cols_to_split = [c for c in df.columns if \"|\" in c and df[c].notna().sum() > 0]\n",
    "        split_frames = []\n",
    "\n",
    "        for c in cols_to_split:\n",
    "            new_cols = c.split(\"|\")\n",
    "            expanded = df[c].astype(str).str.split(\"|\", expand=True)\n",
    "\n",
    "            if expanded.shape[1] == len(new_cols):\n",
    "                expanded.columns = new_cols\n",
    "                split_frames.append(expanded)\n",
    "                df = df.drop(columns=[c])\n",
    "\n",
    "        for f in split_frames:\n",
    "            df = pd.concat([df, f], axis=1)\n",
    "\n",
    "        # Detectar columnas útiles\n",
    "        col_region = detectar_columna_region(df)\n",
    "        if col_region is None:\n",
    "            return None\n",
    "\n",
    "        col_fecha = detectar_columna_fecha(df)\n",
    "        if col_fecha is None:\n",
    "            return None\n",
    "\n",
    "        # Normalizar región\n",
    "        df[col_region] = (\n",
    "            df[col_region]\n",
    "            .astype(str)\n",
    "            .apply(fix_encoding)      # corregir caracteres maldecodificados\n",
    "            .str.normalize(\"NFKD\")    # normalizar tildes\n",
    "            .str.encode(\"ascii\", \"ignore\")  # convertir ñ a n\n",
    "            .str.decode(\"utf8\")\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "        # Filtrar solo regiones relevantes\n",
    "        mask = df[col_region].str.contains(\"bio\") | df[col_region].str.contains(\"nuble\")\n",
    "        df = df[mask]\n",
    "\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # Convertir la fecha a formato YYYY-MM-DD\n",
    "        df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n",
    "\n",
    "        # Eliminar filas sin fecha válida\n",
    "        df = df.dropna(subset=[col_fecha])\n",
    "\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        df[col_fecha] = df[col_fecha].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Normalizar nombres finales de región\n",
    "        df[\"Region\"] = df[col_region].apply(normalizar_region)\n",
    "        df[\"Fecha\"] = df[col_fecha]\n",
    "\n",
    "        df = df[[\"Region\", \"Fecha\"]]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# PROCESAR TODOS LOS DATASETS\n",
    "all_clean_dfs = []\n",
    "\n",
    "for doi in DOIS:\n",
    "    paths = download_dataset_files(doi)\n",
    "\n",
    "    for path in paths:\n",
    "        df_clean = load_and_clean_file(path)\n",
    "        if df_clean is not None:\n",
    "            all_clean_dfs.append(df_clean)\n",
    "\n",
    "if not all_clean_dfs:\n",
    "    raise RuntimeError(\"No se pudo cargar ningún archivo válido con región y fecha.\")\n",
    "\n",
    "# UNIFICAR\n",
    "\n",
    "df_final = pd.concat(all_clean_dfs, ignore_index=True)\n",
    "\n",
    "output_path = os.path.join(DATA_DIR, \"eventos_unificados.csv\")\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Dimensiones finales:\", df_final.shape)\n",
    "print(df_final.head())\n",
    "print(f\"\\nGuardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "def fix_encoding(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Cargar archivo\n",
    "input_file = \"inversion_semestres.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Intentar detectar columna Región\n",
    "def normalize_for_match(s):\n",
    "    #Normaliza texto eliminando tildes y bajando a minúsculas.\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "normalized_cols = {normalize_for_match(c): c for c in df.columns}\n",
    "\n",
    "possible_region_keys = [\n",
    "    \"region\",\n",
    "    \"región\",\n",
    "    \"region administrativa\",\n",
    "    \"nombre region\",\n",
    "    \"nombre región\",\n",
    "]\n",
    "\n",
    "col_region = None\n",
    "for key in possible_region_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_region = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_region is None:\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "    raise KeyError(\"No existe columna equivalente a 'Región' en el archivo.\")\n",
    "\n",
    "# Detectar columna Fecha\n",
    "possible_fecha_keys = [\"fecha\", \"date\", \"fechas\"]\n",
    "\n",
    "col_fecha = None\n",
    "for key in possible_fecha_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_fecha = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_fecha is None:\n",
    "    raise KeyError(\"No se encontró una columna de fecha.\")\n",
    "\n",
    "# Detectar columna Inversión\n",
    "possible_inv_keys = [\n",
    "    \"inversion\",\n",
    "    \"inversión\",\n",
    "    \"inversion (miles de $ de cada año)\",\n",
    "    \"monto inversion\",\n",
    "    \"monto\",\n",
    "]\n",
    "\n",
    "col_inv = None\n",
    "for key in possible_inv_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_inv = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_inv is None:\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "    raise KeyError(\"No existe columna de inversión en el archivo.\")\n",
    "\n",
    "# Normalizar Región\n",
    "df[col_region] = (\n",
    "    df[col_region]\n",
    "    .astype(str)\n",
    "    .apply(fix_encoding)\n",
    "    .str.normalize(\"NFKD\")\n",
    "    .str.encode(\"ascii\", \"ignore\")\n",
    "    .str.decode(\"utf8\")\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Variantes aceptadas\n",
    "bio_vars = [\"bio bio\", \"biobio\", \"bio-bio\"]\n",
    "nuble_vars = [\"nuble\", \"ñuble\"]\n",
    "\n",
    "df_filtered = df[df[col_region].isin(bio_vars + nuble_vars)].copy()\n",
    "\n",
    "# Mapear al nombre final\n",
    "def map_region(r):\n",
    "    if r in bio_vars:\n",
    "        return \"BioBio\"\n",
    "    if r in nuble_vars:\n",
    "        return \"Ñuble\"\n",
    "    return None\n",
    "\n",
    "df_filtered[\"Region\"] = df_filtered[col_region].apply(map_region)\n",
    "\n",
    "# Normalizar fecha\n",
    "df_filtered[\"Fecha\"] = pd.to_datetime(df_filtered[col_fecha], dayfirst=True, errors=\"coerce\").dt.date\n",
    "\n",
    "# Normalizar inversión\n",
    "df_filtered[\"Inversion\"] = (\n",
    "    df_filtered[col_inv]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[^0-9,.\\-]\", \"\", regex=True)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .astype(float)\n",
    "    .mul(1000)\n",
    "    .round(0)        # redondea al entero más cercano\n",
    "    .astype(int)     # convierte a entero\n",
    ")\n",
    "\n",
    "# Exportar CSV\n",
    "output_file = \"inversion_filtrada.csv\"\n",
    "df_filtered[[\"Region\", \"Fecha\", \"Inversion\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\nArchivo generado correctamente:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ARIMAX forecasting (SARIMAX) para regiones BioBio y Ñuble.\n",
    "Entrada:\n",
    " - eventos_unificados.csv  (columns: Region, Fecha)  Fecha puede ser yyyy-mm-dd o similar\n",
    " - inversion_filtrada.csv  (columns: Region, Fecha, Inversion)  Fecha = YYYY-01-01 o YYYY-07-01 (semestres)\n",
    "Salida:\n",
    " - Prints métricas y resumen por región\n",
    " - Guarda gráficos: forecast_BioBio.png y forecast_Ñuble.png (si hay datos)\n",
    " - Guarda CSVs de forecast: forecast_BioBio.csv, forecast_Ñuble.csv\n",
    "Notas:\n",
    " - Forecast: 10 años = 20 semestres\n",
    " - División train/test: 80% train (temporal)\n",
    " - Selección de orden (p,d,q) por búsqueda de AIC con p,q ∈ {0,1,2}, d fijo = 1 (puedes cambiar)\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Configuración \n",
    "\n",
    "EVENTS_FILE = \"datos_eventos/eventos_unificados.csv\"\n",
    "INV_FILE = \"inversion_filtrada.csv\"\n",
    "REGIONS = [\"BioBio\", \"Ñuble\"]  # regiones a procesar\n",
    "FORECAST_YEARS = 10\n",
    "SEMS_PER_YEAR = 2\n",
    "FORECAST_PERIODS = FORECAST_YEARS * SEMS_PER_YEAR  # 20 semestres\n",
    "TRAIN_FRAC = 0.8\n",
    "MAX_P = 2\n",
    "MAX_Q = 2\n",
    "D = 1  # grado de differencing\n",
    "\n",
    "# Utilidades\n",
    "def normalize_region_name(s):\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    s = str(s).strip()\n",
    "    low = s.lower()\n",
    "    if \"bio\" in low:\n",
    "        return \"BioBio\"\n",
    "    if \"ñuble\" in low or \"nuble\" in low:\n",
    "        return \"Ñuble\"\n",
    "    return s\n",
    "\n",
    "def to_semester_start(ts):\n",
    "    ts = pd.to_datetime(ts, errors='coerce')\n",
    "    if pd.isna(ts):\n",
    "        return pd.NaT\n",
    "    if ts.month <= 6:\n",
    "        return pd.Timestamp(year=ts.year, month=1, day=1)\n",
    "    else:\n",
    "        return pd.Timestamp(year=ts.year, month=7, day=1)\n",
    "\n",
    "def load_data():\n",
    "    if not os.path.exists(EVENTS_FILE):\n",
    "        raise FileNotFoundError(f\"No encuentro {EVENTS_FILE} en el directorio actual.\")\n",
    "    if not os.path.exists(INV_FILE):\n",
    "        raise FileNotFoundError(f\"No encuentro {INV_FILE} en el directorio actual.\")\n",
    "    eventos = pd.read_csv(EVENTS_FILE)\n",
    "    inversion = pd.read_csv(INV_FILE)\n",
    "    return eventos, inversion\n",
    "\n",
    "def prepare(eventos, inversion):\n",
    "    # Normalizar nombres\n",
    "    eventos['Region'] = eventos['Region'].apply(normalize_region_name)\n",
    "    inversion['Region'] = inversion['Region'].apply(normalize_region_name)\n",
    "    # Parse fecha\n",
    "    eventos['Fecha'] = pd.to_datetime(eventos['Fecha'], errors='coerce')\n",
    "    inversion['Fecha'] = pd.to_datetime(inversion['Fecha'], errors='coerce')\n",
    "    # Mapear eventos a semestre (semester start)\n",
    "    eventos['Semestre'] = eventos['Fecha'].apply(to_semester_start)\n",
    "    # Contar eventos por region y semestre\n",
    "    ev_counts = (eventos\n",
    "                 .dropna(subset=['Semestre'])\n",
    "                 .groupby(['Region','Semestre'])\n",
    "                 .size()\n",
    "                 .reset_index(name='Eventos'))\n",
    "    ev_counts = ev_counts.rename(columns={'Semestre':'Fecha'})\n",
    "    # Asegurar que inversion tenga semestres normalizados (redondear al inicio de semestre)\n",
    "    inversion['Fecha'] = inversion['Fecha'].apply(to_semester_start)\n",
    "    # Agregar inversion por region+fecha (si hay duplicados)\n",
    "    inv_agg = inversion.groupby(['Region','Fecha'], as_index=False)['Inversion'].sum()\n",
    "    # Merge outer para que entren semestres faltantes\n",
    "    merged = pd.merge(inv_agg, ev_counts, on=['Region','Fecha'], how='outer')\n",
    "    merged['Inversion'] = merged['Inversion'].fillna(0.0)\n",
    "    merged['Eventos'] = merged['Eventos'].fillna(0).astype(int)\n",
    "    merged = merged.sort_values(['Region','Fecha']).reset_index(drop=True)\n",
    "    return merged\n",
    "\n",
    "def ensure_semester_index(df_region):\n",
    "    # df_region: tiene columnas Fecha, Inversion, Eventos para una region\n",
    "    df_region = df_region.set_index('Fecha').sort_index()\n",
    "    # crear rango completo desde primer a ultimo semestre con freq '6MS' anclado en inicio de semestre\n",
    "    start = df_region.index.min()\n",
    "    end = df_region.index.max()\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        return pd.DataFrame(columns=['Inversion','Eventos'])\n",
    "    idx = pd.date_range(start=start, end=end, freq='6MS')  # cada semestre\n",
    "    s = df_region.reindex(idx)\n",
    "    s.index.name = 'Fecha'\n",
    "    s['Inversion'] = s['Inversion'].fillna(0.0)\n",
    "    s['Eventos'] = s['Eventos'].fillna(0).astype(int)\n",
    "    return s\n",
    "\n",
    "def select_order_by_aic(endog, exog, seasonal_order, max_p=2, max_q=2, d=1):\n",
    "    best_aic = np.inf\n",
    "    best_order = (1, d, 1) # Default fallback\n",
    "    \n",
    "    for p in range(0, max_p + 1):\n",
    "        for q in range(0, max_q + 1):\n",
    "            try:\n",
    "                # Probamos el modelo COMPLETO (seasonal + non-seasonal)\n",
    "                # Si hay conflicto de lags (ej: q=2 con s=2), SARIMAX lanzará error aquí\n",
    "                # y el except lo capturará, saltando esa combinación inválida.\n",
    "                model = SARIMAX(endog, exog=exog, \n",
    "                                order=(p, d, q),\n",
    "                                seasonal_order=seasonal_order,\n",
    "                                enforce_stationarity=False, \n",
    "                                enforce_invertibility=False)\n",
    "                \n",
    "                res = model.fit(disp=False, maxiter=200)\n",
    "                \n",
    "                if res.aic < best_aic:\n",
    "                    best_aic = res.aic\n",
    "                    best_order = (p, d, q)\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "    return best_order\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denom = np.where(np.abs(y_true) < 1e-9, 1.0, np.abs(y_true))\n",
    "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
    "\n",
    "# Pipeline por región\n",
    "def run_for_region(merged, region, forecast_periods=FORECAST_PERIODS):\n",
    "    df_reg = merged[merged['Region'] == region][['Fecha','Inversion','Eventos']].copy()\n",
    "    if df_reg.empty:\n",
    "        return None\n",
    "    df_reg['Fecha'] = pd.to_datetime(df_reg['Fecha'])\n",
    "    series = ensure_semester_index(df_reg)\n",
    "    if series.empty:\n",
    "        return None\n",
    "\n",
    "    # División train/test temporal (80% train)\n",
    "    n = len(series)\n",
    "    train_n = int(np.floor(n * TRAIN_FRAC))\n",
    "    if train_n < 4:\n",
    "        train_n = max(1, n - 2)\n",
    "    train_idx = series.index[:train_n]\n",
    "    test_idx = series.index[train_n:]\n",
    "\n",
    "    endog = series['Inversion']\n",
    "    exog = series[['Eventos']]\n",
    "\n",
    "    # Define estacionalidad semestral\n",
    "    # (P=1, D=0, Q=1, s=2)\n",
    "    s_order = (1, 0, 1, 2)\n",
    "\n",
    "    # Selección de orden por AIC\n",
    "    try:\n",
    "        # s_order pasa a la función \n",
    "        order = select_order_by_aic(endog.loc[train_idx], exog.loc[train_idx], \n",
    "                                    seasonal_order=s_order, \n",
    "                                    max_p=MAX_P, max_q=MAX_Q, d=D)\n",
    "    except Exception as e:\n",
    "        order = (1, D, 1)\n",
    "\n",
    "    # Ajuste modelo \n",
    "    model = SARIMAX(endog.loc[train_idx], \n",
    "                    exog=exog.loc[train_idx], \n",
    "                    order=order,\n",
    "                    seasonal_order=s_order, \n",
    "                    enforce_stationarity=False, \n",
    "                    enforce_invertibility=False)\n",
    "    \n",
    "    res = model.fit(disp=False, maxiter=200)\n",
    "\n",
    "    metrics = {}\n",
    "    if len(test_idx) > 0:\n",
    "        pred_test_res = res.get_prediction(start=test_idx[0], end=test_idx[-1], exog=exog.loc[test_idx], dynamic=False)\n",
    "        pred_test = pred_test_res.predicted_mean\n",
    "        true_test = endog.loc[test_idx]\n",
    "\n",
    "        # MSE \n",
    "        mse = mean_squared_error(true_test, pred_test)\n",
    "\n",
    "        # RMSE \n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        # MRE \n",
    "        try:\n",
    "            mre = np.mean(np.abs((true_test - pred_test) / true_test))\n",
    "        except:\n",
    "            mre = None\n",
    "\n",
    "        metrics = {\n",
    "            'MSE': mse, \n",
    "            'RMSE': rmse, \n",
    "            'MRE': mre,\n",
    "            'MAPE (%)': mre * 100 if mre is not None else None \n",
    "        }\n",
    "    else:\n",
    "        pred_test = pd.Series(dtype=float)\n",
    "        metrics = {'MSE': None, 'RMSE': None, 'MRE': None}\n",
    "\n",
    "    # Forecast futuro\n",
    "    last_cycle = exog.iloc[-4:]['Eventos'].values \n",
    "    num_cycles = int(np.ceil(forecast_periods / len(last_cycle)))\n",
    "    future_exog_vals = np.tile(last_cycle, num_cycles)[:forecast_periods]\n",
    "    \n",
    "    start_future = series.index[-1] + pd.DateOffset(months=6)\n",
    "    future_index = pd.date_range(start=start_future, periods=forecast_periods, freq='6MS')\n",
    "\n",
    "    forecast_res = res.get_forecast(steps=forecast_periods, exog=future_exog_vals.reshape(-1,1))\n",
    "    forecast_mean = forecast_res.predicted_mean\n",
    "    forecast_ci = forecast_res.conf_int()\n",
    "\n",
    "    df_forecast = pd.DataFrame({\n",
    "        'Fecha': future_index,\n",
    "        'Forecast': forecast_mean.values,\n",
    "        'Lower_CI': forecast_ci.iloc[:,0].values,\n",
    "        'Upper_CI': forecast_ci.iloc[:,1].values,\n",
    "        'Exog_Pred_Eventos': future_exog_vals\n",
    "    }).set_index('Fecha')\n",
    "\n",
    "    out = {\n",
    "        'region': region,\n",
    "        'order': order,\n",
    "        'model_result': res,\n",
    "        'train_index': train_idx,\n",
    "        'test_index': test_idx,\n",
    "        'pred_test': pred_test,\n",
    "        'true_test': endog.loc[test_idx],\n",
    "        'metrics': metrics,\n",
    "        'forecast_df': df_forecast,\n",
    "        'series': series\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    try:\n",
    "        eventos, inversion = load_data()\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    merged = prepare(eventos, inversion)\n",
    "    # Asegurar que Fecha sea datetime\n",
    "    merged['Fecha'] = pd.to_datetime(merged['Fecha'])\n",
    "    # Procesar cada región\n",
    "    results = {}\n",
    "    for region in REGIONS:\n",
    "        res = run_for_region(merged, region)\n",
    "        if res is None:\n",
    "            continue\n",
    "        results[region] = res\n",
    "\n",
    "        # Imprimir métricas\n",
    "        print(f\"\\n=== Resultados para {region} ===\")\n",
    "        print(\"Order (p,d,q):\", res['order'])\n",
    "        print(\"Métricas (test):\", res['metrics'])\n",
    "        # resumen del modelo (primeras tablas)\n",
    "        try:\n",
    "            print(res['model_result'].summary().tables[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Guardar forecast a CSV\n",
    "        fname = f\"forecast_{region}.csv\"\n",
    "        res['forecast_df'].to_csv(fname, index=True)\n",
    "        print(f\"Forecast guardado en {fname}\")\n",
    "\n",
    "        # Graficar observed, test pred, forecast\n",
    "        plt.figure(figsize=(12,6))\n",
    "        series = res['series']\n",
    "        plt.plot(series.index, series['Inversion'], label='Observed (Inversion)', marker='o')\n",
    "        if len(res['test_index'])>0 and not res['pred_test'].empty:\n",
    "            plt.plot(res['pred_test'].index, res['pred_test'].values, label='Predicted (test)', linestyle='--', marker='x')\n",
    "        plt.plot(res['forecast_df'].index, res['forecast_df']['Forecast'], label=f'Forecast ({FORECAST_YEARS} yrs)', marker='s')\n",
    "        plt.fill_between(res['forecast_df'].index, res['forecast_df']['Lower_CI'], res['forecast_df']['Upper_CI'], alpha=0.25)\n",
    "        plt.title(f\"ARIMAX Forecast - {region}\")\n",
    "        plt.xlabel(\"Fecha (semestre)\")\n",
    "        plt.ylabel(\"Inversión\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        imgname = f\"forecast_{region}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(imgname)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
