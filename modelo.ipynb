{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d8865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/FMGXND\n",
      "===================================\n",
      "\n",
      " Archivo: 2015.csv, id=302\n",
      "Guardado en datos_eventos\\2015.csv\n",
      " Archivo: 2016.csv, id=303\n",
      "Guardado en datos_eventos\\2016.csv\n",
      " Archivo: 2017.csv, id=304\n",
      "Guardado en datos_eventos\\2017.csv\n",
      " Archivo: 2018.csv, id=305\n",
      "Guardado en datos_eventos\\2018.csv\n",
      " Archivo: - Archivo Indice.csv, id=306\n",
      "Guardado en datos_eventos\\- Archivo Indice.csv\n",
      "Omitido (sin columna región): datos_eventos\\- Archivo Indice.csv\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/UXAUN5\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mate\\AppData\\Local\\Temp\\ipykernel_21852\\1018215776.py:192: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n",
      "C:\\Users\\mate\\AppData\\Local\\Temp\\ipykernel_21852\\1018215776.py:192: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Archivo: 20022003.csv, id=5234\n",
      "Guardado en datos_eventos\\20022003.csv\n",
      " Archivo: 20022003.geojson, id=5238\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20022003.zip, id=5231\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20032004.csv, id=5434\n",
      "Guardado en datos_eventos\\20032004.csv\n",
      " Archivo: 20032004.geojson, id=5436\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20032004.zip, id=5430\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20042005.csv, id=5225\n",
      "Guardado en datos_eventos\\20042005.csv\n",
      " Archivo: 20042005.geojson, id=5228\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20042005.zip, id=5223\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20052006.csv, id=5375\n",
      "Guardado en datos_eventos\\20052006.csv\n",
      " Archivo: 20052006.geojson, id=5378\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20052006.zip, id=5363\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20062007.csv, id=5300\n",
      "Guardado en datos_eventos\\20062007.csv\n",
      " Archivo: 20062007.geojson, id=5304\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20062007.zip, id=5298\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20072008.csv, id=5416\n",
      "Guardado en datos_eventos\\20072008.csv\n",
      " Archivo: 20072008.geojson, id=5420\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20072008.zip, id=5413\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20082009.csv, id=5455\n",
      "Guardado en datos_eventos\\20082009.csv\n",
      " Archivo: 20082009.geojson, id=5458\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20082009.zip, id=5440\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20092010.csv, id=5355\n",
      "Guardado en datos_eventos\\20092010.csv\n",
      " Archivo: 20092010.geojson, id=5359\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20092010.zip, id=5343\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20102011.csv, id=5289\n",
      "Guardado en datos_eventos\\20102011.csv\n",
      " Archivo: 20102011.geojson, id=5293\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20102011.zip, id=5285\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20112012.csv, id=5256\n",
      "Guardado en datos_eventos\\20112012.csv\n",
      " Archivo: 20112012.geojson, id=5261\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20112012.zip, id=5254\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20122013.csv, id=5336\n",
      "Guardado en datos_eventos\\20122013.csv\n",
      " Archivo: 20122013.geojson, id=5340\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20122013.zip, id=5331\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20132014.csv, id=5246\n",
      "Guardado en datos_eventos\\20132014.csv\n",
      " Archivo: 20132014.geojson, id=5250\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20132014.zip, id=5242\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20142015.csv, id=5385\n",
      "Guardado en datos_eventos\\20142015.csv\n",
      " Archivo: 20142015.geojson, id=5392\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20142015.zip, id=5381\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20152016.csv, id=5266\n",
      "Guardado en datos_eventos\\20152016.csv\n",
      " Archivo: 20152016.geojson, id=5271\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20152016.zip, id=5264\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20162017.csv, id=5311\n",
      "Guardado en datos_eventos\\20162017.csv\n",
      " Archivo: 20162017.geojson, id=5314\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20162017.zip, id=5309\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20172018.csv, id=5401\n",
      "Guardado en datos_eventos\\20172018.csv\n",
      " Archivo: 20172018.geojson, id=5405\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20172018.zip, id=5399\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20182019.csv, id=5278\n",
      "Guardado en datos_eventos\\20182019.csv\n",
      " Archivo: 20182019.geojson, id=5282\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20182019.zip, id=5275\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20192020.csv, id=5323\n",
      "Guardado en datos_eventos\\20192020.csv\n",
      " Archivo: 20192020.geojson, id=5326\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20192020.zip, id=5318\n",
      "   → No tabular, omitido.\n",
      " Archivo: - Archivo Indice.csv, id=5361\n",
      "Guardado en datos_eventos\\- Archivo Indice.csv\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "Omitido (sin columna región): datos_eventos\\- Archivo Indice.csv\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/QRDY49\n",
      "===================================\n",
      "\n",
      " Archivo: catastro.csv, id=757\n",
      "Guardado en datos_eventos\\catastro.csv\n",
      " Archivo: catastro.geojson, id=756\n",
      "   → No tabular, omitido.\n",
      " Archivo: catastro.xlsx, id=755\n",
      "Guardado en datos_eventos\\catastro.xlsx\n",
      " Archivo: catastro.zip, id=758\n",
      "   → No tabular, omitido.\n",
      "Error leyendo/limpiando datos_eventos\\catastro.csv Error tokenizing data. C error: Expected 4 fields in line 6, saw 5\n",
      "\n",
      "Omitido (sin columna fecha): datos_eventos\\catastro.xlsx\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/HKLGZ8\n",
      "===================================\n",
      "\n",
      " Archivo: catastro.csv, id=625\n",
      "Guardado en datos_eventos\\catastro.csv\n",
      " Archivo: catastro.geojson, id=624\n",
      "   → No tabular, omitido.\n",
      " Archivo: catastro.kml, id=626\n",
      "   → No tabular, omitido.\n",
      " Archivo: catastro.xlsx, id=623\n",
      "Guardado en datos_eventos\\catastro.xlsx\n",
      " Archivo: catastro.zip, id=627\n",
      "   → No tabular, omitido.\n",
      "Omitido (sin columna fecha): datos_eventos\\catastro.csv\n",
      "Omitido (sin columna fecha): datos_eventos\\catastro.xlsx\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/TLSNSV\n",
      "===================================\n",
      "\n",
      " Archivo: - Archivo Indice.csv, id=776\n",
      "Guardado en datos_eventos\\- Archivo Indice.csv\n",
      " Archivo: gorum2014.csv, id=774\n",
      "Guardado en datos_eventos\\gorum2014.csv\n",
      " Archivo: gorum2014.geojson, id=773\n",
      "   → No tabular, omitido.\n",
      " Archivo: gorum2014.mat, id=770\n",
      "   → No tabular, omitido.\n",
      " Archivo: gorum2014.npz, id=771\n",
      "   → No tabular, omitido.\n",
      " Archivo: gorum2014.xlsx, id=772\n",
      "Guardado en datos_eventos\\gorum2014.xlsx\n",
      " Archivo: gorum2014.zip, id=775\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.csv, id=768\n",
      "Guardado en datos_eventos\\sepulveda2010.csv\n",
      " Archivo: sepulveda2010.geojson, id=767\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.mat, id=764\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.npz, id=765\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.xlsx, id=766\n",
      "Guardado en datos_eventos\\sepulveda2010.xlsx\n",
      " Archivo: sepulveda2010.zip, id=769\n",
      "   → No tabular, omitido.\n",
      "→ Separando columna fusionada: PUBLICATION|LANDSLIDES|EVENT_DATE|COMMENTS|REFERENCES\n",
      "Omitido (sin columna región): datos_eventos\\- Archivo Indice.csv\n",
      "Omitido (sin columna región): datos_eventos\\gorum2014.csv\n",
      "Omitido (sin columna región): datos_eventos\\gorum2014.xlsx\n",
      "Omitido (sin columna región): datos_eventos\\sepulveda2010.csv\n",
      "Omitido (sin columna región): datos_eventos\\sepulveda2010.xlsx\n",
      "\n",
      "===================================\n",
      "ARCHIVO FINAL GENERADO\n",
      "===================================\n",
      "Dimensiones finales: (46297, 2)\n",
      "   Region       Fecha\n",
      "0  BioBio  2015-12-31\n",
      "1  BioBio  2015-12-31\n",
      "2  BioBio  2016-02-23\n",
      "3  BioBio  2016-02-23\n",
      "4  BioBio  2016-02-28\n",
      "\n",
      "Guardado en: datos_eventos\\eventos_unificados.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyDataverse.api import NativeApi, DataAccessApi\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURACIÓN\n",
    "# ==========================================\n",
    "\n",
    "API_KEY = \"7386f7e6-18cc-49ee-a8eb-d08ac888783d\"\n",
    "BASE_URL = \"https://datospararesiliencia.cl\"\n",
    "\n",
    "DATA_DIR = \"datos_eventos\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "DOIS = [\n",
    "    \"doi:10.71578/FMGXND\",\n",
    "    \"doi:10.71578/UXAUN5\",\n",
    "    \"doi:10.71578/QRDY49\",\n",
    "    \"doi:10.71578/HKLGZ8\",\n",
    "    \"doi:10.71578/TLSNSV\",\n",
    "]\n",
    "\n",
    "VALID_EXT = {\".csv\", \".xlsx\", \".xls\"}\n",
    "\n",
    "\n",
    "api = NativeApi(BASE_URL, API_KEY)\n",
    "data_api = DataAccessApi(BASE_URL, API_KEY)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# UTILIDADES\n",
    "# ==========================================\n",
    "\n",
    "# --- 1: corregir caracteres maldecodificados ---\n",
    "def fix_encoding(col):\n",
    "    try:\n",
    "        return col.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return col\n",
    "\n",
    "\n",
    "# Detectar columna Región\n",
    "def detectar_columna_region(df):\n",
    "    for col in df.columns:\n",
    "        c = col.lower().strip()\n",
    "        if \"regi\" in c:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# Detectar columna Fecha\n",
    "def detectar_columna_fecha(df):\n",
    "    posibles = [\"fecha\", \"fecha_update\"]\n",
    "    for col in df.columns:\n",
    "        if col.lower().strip() in posibles:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# Normalizar nombre de región final\n",
    "def normalizar_region(valor):\n",
    "    v = str(valor).lower()\n",
    "    if \"bio\" in v:\n",
    "        return \"BioBio\"\n",
    "    if \"nuble\" in v:\n",
    "        return \"Ñuble\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# API: descargar archivos\n",
    "# ==========================================\n",
    "def download_dataset_files(doi):\n",
    "    print(\"\\n===================================\")\n",
    "    print(\"Procesando dataset:\", doi)\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    ds = api.get_dataset(doi)\n",
    "\n",
    "    if ds.status_code != 200:\n",
    "        print(\"Error obteniendo dataset:\", doi)\n",
    "        return []\n",
    "\n",
    "    files = ds.json()[\"data\"][\"latestVersion\"][\"files\"]\n",
    "    downloaded_paths = []\n",
    "\n",
    "    for f in files:\n",
    "        fname = f[\"dataFile\"][\"filename\"]\n",
    "        fid = f[\"dataFile\"][\"id\"]\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "\n",
    "        print(f\" Archivo: {fname}, id={fid}\")\n",
    "\n",
    "        if ext not in VALID_EXT:\n",
    "            print(\"   → No tabular, omitido.\")\n",
    "            continue\n",
    "\n",
    "        resp = data_api.get_datafile(fid, is_pid=False)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"Error descargando\", fname)\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(DATA_DIR, fname)\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "\n",
    "        print(\"Guardado en\", path)\n",
    "        downloaded_paths.append(path)\n",
    "\n",
    "    return downloaded_paths\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PROCESAR Y LIMPIAR TABLAS\n",
    "# ==========================================\n",
    "def load_and_clean_file(path):\n",
    "    try:\n",
    "        # -------------------------\n",
    "        # 1. Cargar archivo\n",
    "        # -------------------------\n",
    "        if path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "        elif path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "            df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Corregir encoding de columnas\n",
    "        # -------------------------\n",
    "        df.columns = [fix_encoding(c) for c in df.columns]\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Separar columnas fusionadas con \"|\"\n",
    "        # -------------------------\n",
    "        cols_to_split = [c for c in df.columns if \"|\" in c and df[c].notna().sum() > 0]\n",
    "        split_frames = []\n",
    "\n",
    "        for c in cols_to_split:\n",
    "            print(f\"→ Separando columna fusionada: {c}\")\n",
    "            new_cols = c.split(\"|\")\n",
    "            expanded = df[c].astype(str).str.split(\"|\", expand=True)\n",
    "\n",
    "            if expanded.shape[1] == len(new_cols):\n",
    "                expanded.columns = new_cols\n",
    "                split_frames.append(expanded)\n",
    "                df = df.drop(columns=[c])\n",
    "\n",
    "        for f in split_frames:\n",
    "            df = pd.concat([df, f], axis=1)\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Detectar columnas útiles\n",
    "        # -------------------------\n",
    "        col_region = detectar_columna_region(df)\n",
    "        if col_region is None:\n",
    "            print(f\"Omitido (sin columna región): {path}\")\n",
    "            return None\n",
    "\n",
    "        col_fecha = detectar_columna_fecha(df)\n",
    "        if col_fecha is None:\n",
    "            print(f\"Omitido (sin columna fecha): {path}\")\n",
    "            return None\n",
    "\n",
    "        # -------------------------\n",
    "        # 5. Normalizar región\n",
    "        # -------------------------\n",
    "        df[col_region] = (\n",
    "            df[col_region]\n",
    "            .astype(str)\n",
    "            .apply(fix_encoding)      # corregir caracteres maldecodificados\n",
    "            .str.normalize(\"NFKD\")    # normalizar tildes\n",
    "            .str.encode(\"ascii\", \"ignore\")  # convertir Ñ -> n si fuera necesario\n",
    "            .str.decode(\"utf8\")\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # 6. Filtrar solo regiones relevantes\n",
    "        # -------------------------\n",
    "        mask = df[col_region].str.contains(\"bio\") | df[col_region].str.contains(\"nuble\")\n",
    "        df = df[mask]\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"   ⚠ Archivo sin filas de Biobío/Ñuble: {path}\")\n",
    "            return None\n",
    "\n",
    "        # -------------------------\n",
    "        # 7. Convertir la fecha a formato YYYY-MM-DD\n",
    "        # -------------------------\n",
    "        df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n",
    "\n",
    "        # eliminar filas sin fecha válida\n",
    "        df = df.dropna(subset=[col_fecha])\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"   ⚠ Archivo sin fechas válidas después de normalizar: {path}\")\n",
    "            return None\n",
    "\n",
    "        df[col_fecha] = df[col_fecha].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 8. Normalizar nombres finales de región\n",
    "        # -------------------------\n",
    "        df[\"Region\"] = df[col_region].apply(normalizar_region)\n",
    "        df[\"Fecha\"] = df[col_fecha]\n",
    "\n",
    "        df = df[[\"Region\", \"Fecha\"]]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error leyendo/limpiando\", path, e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PROCESAR TODOS LOS DATASETS\n",
    "# ==========================================\n",
    "\n",
    "all_clean_dfs = []\n",
    "\n",
    "for doi in DOIS:\n",
    "    paths = download_dataset_files(doi)\n",
    "\n",
    "    for path in paths:\n",
    "        df_clean = load_and_clean_file(path)\n",
    "        if df_clean is not None:\n",
    "            all_clean_dfs.append(df_clean)\n",
    "\n",
    "if not all_clean_dfs:\n",
    "    raise RuntimeError(\"No se pudo cargar ningún archivo válido con región y fecha.\")\n",
    "\n",
    "# ==========================================\n",
    "# UNIFICAR\n",
    "# ==========================================\n",
    "\n",
    "df_final = pd.concat(all_clean_dfs, ignore_index=True)\n",
    "\n",
    "output_path = os.path.join(DATA_DIR, \"eventos_unificados.csv\")\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n===================================\")\n",
    "print(\"ARCHIVO FINAL GENERADO\")\n",
    "print(\"===================================\")\n",
    "print(\"Dimensiones finales:\", df_final.shape)\n",
    "print(df_final.head())\n",
    "print(f\"\\nGuardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d24e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna Región detectada como: REGIÓN\n",
      "Columna Fecha detectada como: Fecha\n",
      "Columna Inversión detectada como: INVERSIÓN (MILES DE $ DE CADA AÑO)\n",
      "\n",
      "Archivo generado correctamente: inversion_filtrada.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "def fix_encoding(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Cargar archivo\n",
    "# -----------------------------------------\n",
    "input_file = \"inversion_semestres.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. Intentar detectar columna Región\n",
    "# -----------------------------------------\n",
    "def normalize_for_match(s):\n",
    "    \"\"\"Normaliza texto eliminando tildes y bajando a minúsculas.\"\"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "normalized_cols = {normalize_for_match(c): c for c in df.columns}\n",
    "\n",
    "possible_region_keys = [\n",
    "    \"region\",\n",
    "    \"región\",\n",
    "    \"region administrativa\",\n",
    "    \"nombre region\",\n",
    "    \"nombre región\",\n",
    "]\n",
    "\n",
    "col_region = None\n",
    "for key in possible_region_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_region = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_region is None:\n",
    "    print(\"\\nNo se encontró la columna Región. Columnas disponibles:\")\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "    raise KeyError(\"No existe columna equivalente a 'Región' en el archivo.\")\n",
    "\n",
    "print(\"Columna Región detectada como:\", col_region)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Detectar columna Fecha\n",
    "# -----------------------------------------\n",
    "possible_fecha_keys = [\"fecha\", \"date\", \"fechas\"]\n",
    "\n",
    "col_fecha = None\n",
    "for key in possible_fecha_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_fecha = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_fecha is None:\n",
    "    raise KeyError(\"No se encontró una columna de fecha.\")\n",
    "\n",
    "print(\"Columna Fecha detectada como:\", col_fecha)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Detectar columna Inversión\n",
    "# -----------------------------------------\n",
    "possible_inv_keys = [\n",
    "    \"inversion\",\n",
    "    \"inversión\",\n",
    "    \"inversion (miles de $ de cada año)\",\n",
    "    \"monto inversion\",\n",
    "    \"monto\",\n",
    "]\n",
    "\n",
    "col_inv = None\n",
    "for key in possible_inv_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_inv = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_inv is None:\n",
    "    print(\"\\nNo se encontró la columna de inversión. Columnas disponibles:\")\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "    raise KeyError(\"No existe columna de inversión en el archivo.\")\n",
    "\n",
    "print(\"Columna Inversión detectada como:\", col_inv)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5. Normalizar Región\n",
    "# -----------------------------------------\n",
    "df[col_region] = (\n",
    "    df[col_region]\n",
    "    .astype(str)\n",
    "    .apply(fix_encoding)\n",
    "    .str.normalize(\"NFKD\")\n",
    "    .str.encode(\"ascii\", \"ignore\")\n",
    "    .str.decode(\"utf8\")\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Variantes aceptadas\n",
    "bio_vars = [\"bio bio\", \"biobio\", \"bio-bio\"]\n",
    "nuble_vars = [\"nuble\", \"ñuble\"]\n",
    "\n",
    "df_filtered = df[df[col_region].isin(bio_vars + nuble_vars)].copy()\n",
    "\n",
    "# Mapear al nombre final\n",
    "def map_region(r):\n",
    "    if r in bio_vars:\n",
    "        return \"BioBio\"\n",
    "    if r in nuble_vars:\n",
    "        return \"Ñuble\"\n",
    "    return None\n",
    "\n",
    "df_filtered[\"Region\"] = df_filtered[col_region].apply(map_region)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Normalizar fecha\n",
    "# -----------------------------------------\n",
    "df_filtered[\"Fecha\"] = pd.to_datetime(df_filtered[col_fecha], dayfirst=True, errors=\"coerce\").dt.date\n",
    "\n",
    "# -----------------------------------------\n",
    "# Normalizar inversión\n",
    "# -----------------------------------------\n",
    "df_filtered[\"Inversion\"] = (\n",
    "    df_filtered[col_inv]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[^0-9,.\\-]\", \"\", regex=True)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .astype(float)\n",
    "    .mul(1000)\n",
    "    .round(0)        # redondea al entero más cercano\n",
    "    .astype(int)     # convierte a entero\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Exportar CSV\n",
    "# -----------------------------------------\n",
    "output_file = \"inversion_filtrada.csv\"\n",
    "df_filtered[[\"Region\", \"Fecha\", \"Inversion\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\nArchivo generado correctamente:\", output_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
