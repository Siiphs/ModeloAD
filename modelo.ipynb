{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d8865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/FMGXND\n",
      "===================================\n",
      "\n",
      " Archivo: 2015.csv, id=302\n",
      "Guardado en datos_eventos\\2015.csv\n",
      " Archivo: 2016.csv, id=303\n",
      "Guardado en datos_eventos\\2016.csv\n",
      " Archivo: 2017.csv, id=304\n",
      "Guardado en datos_eventos\\2017.csv\n",
      " Archivo: 2018.csv, id=305\n",
      "Guardado en datos_eventos\\2018.csv\n",
      " Archivo: - Archivo Indice.csv, id=306\n",
      "Guardado en datos_eventos\\- Archivo Indice.csv\n",
      "Omitido (sin columna región): datos_eventos\\- Archivo Indice.csv\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/UXAUN5\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mate\\AppData\\Local\\Temp\\ipykernel_21852\\1018215776.py:192: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n",
      "C:\\Users\\mate\\AppData\\Local\\Temp\\ipykernel_21852\\1018215776.py:192: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Archivo: 20022003.csv, id=5234\n",
      "Guardado en datos_eventos\\20022003.csv\n",
      " Archivo: 20022003.geojson, id=5238\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20022003.zip, id=5231\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20032004.csv, id=5434\n",
      "Guardado en datos_eventos\\20032004.csv\n",
      " Archivo: 20032004.geojson, id=5436\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20032004.zip, id=5430\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20042005.csv, id=5225\n",
      "Guardado en datos_eventos\\20042005.csv\n",
      " Archivo: 20042005.geojson, id=5228\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20042005.zip, id=5223\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20052006.csv, id=5375\n",
      "Guardado en datos_eventos\\20052006.csv\n",
      " Archivo: 20052006.geojson, id=5378\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20052006.zip, id=5363\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20062007.csv, id=5300\n",
      "Guardado en datos_eventos\\20062007.csv\n",
      " Archivo: 20062007.geojson, id=5304\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20062007.zip, id=5298\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20072008.csv, id=5416\n",
      "Guardado en datos_eventos\\20072008.csv\n",
      " Archivo: 20072008.geojson, id=5420\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20072008.zip, id=5413\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20082009.csv, id=5455\n",
      "Guardado en datos_eventos\\20082009.csv\n",
      " Archivo: 20082009.geojson, id=5458\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20082009.zip, id=5440\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20092010.csv, id=5355\n",
      "Guardado en datos_eventos\\20092010.csv\n",
      " Archivo: 20092010.geojson, id=5359\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20092010.zip, id=5343\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20102011.csv, id=5289\n",
      "Guardado en datos_eventos\\20102011.csv\n",
      " Archivo: 20102011.geojson, id=5293\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20102011.zip, id=5285\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20112012.csv, id=5256\n",
      "Guardado en datos_eventos\\20112012.csv\n",
      " Archivo: 20112012.geojson, id=5261\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20112012.zip, id=5254\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20122013.csv, id=5336\n",
      "Guardado en datos_eventos\\20122013.csv\n",
      " Archivo: 20122013.geojson, id=5340\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20122013.zip, id=5331\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20132014.csv, id=5246\n",
      "Guardado en datos_eventos\\20132014.csv\n",
      " Archivo: 20132014.geojson, id=5250\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20132014.zip, id=5242\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20142015.csv, id=5385\n",
      "Guardado en datos_eventos\\20142015.csv\n",
      " Archivo: 20142015.geojson, id=5392\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20142015.zip, id=5381\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20152016.csv, id=5266\n",
      "Guardado en datos_eventos\\20152016.csv\n",
      " Archivo: 20152016.geojson, id=5271\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20152016.zip, id=5264\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20162017.csv, id=5311\n",
      "Guardado en datos_eventos\\20162017.csv\n",
      " Archivo: 20162017.geojson, id=5314\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20162017.zip, id=5309\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20172018.csv, id=5401\n",
      "Guardado en datos_eventos\\20172018.csv\n",
      " Archivo: 20172018.geojson, id=5405\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20172018.zip, id=5399\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20182019.csv, id=5278\n",
      "Guardado en datos_eventos\\20182019.csv\n",
      " Archivo: 20182019.geojson, id=5282\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20182019.zip, id=5275\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20192020.csv, id=5323\n",
      "Guardado en datos_eventos\\20192020.csv\n",
      " Archivo: 20192020.geojson, id=5326\n",
      "   → No tabular, omitido.\n",
      " Archivo: 20192020.zip, id=5318\n",
      "   → No tabular, omitido.\n",
      " Archivo: - Archivo Indice.csv, id=5361\n",
      "Guardado en datos_eventos\\- Archivo Indice.csv\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "→ Separando columna fusionada: Región|Provincia|Comuna|Temporada|Nombre|Fecha|Hora inicio|Duración (minutos)|Alerta|Escenario|Causa|Superficie quemada: Pino A [ha]|Superficie quemada: Pino B [ha]|Superficie quemada: Pino C [ha]|Superficie quemada: Eucalípto [ha]|Superficie quemada: Otras plantas [ha]|Superficie quemada: Arbolado [ha]|Superficie quemada: Matorral [ha]|Superficie quemada: Pastizal [ha]|Superficie quemada: Agrícola [ha]|Superficie quemada: Desechos [ha]|Superficie quemada total [ha]|Latitud|Longitud|Datum\n",
      "Omitido (sin columna región): datos_eventos\\- Archivo Indice.csv\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/QRDY49\n",
      "===================================\n",
      "\n",
      " Archivo: catastro.csv, id=757\n",
      "Guardado en datos_eventos\\catastro.csv\n",
      " Archivo: catastro.geojson, id=756\n",
      "   → No tabular, omitido.\n",
      " Archivo: catastro.xlsx, id=755\n",
      "Guardado en datos_eventos\\catastro.xlsx\n",
      " Archivo: catastro.zip, id=758\n",
      "   → No tabular, omitido.\n",
      "Error leyendo/limpiando datos_eventos\\catastro.csv Error tokenizing data. C error: Expected 4 fields in line 6, saw 5\n",
      "\n",
      "Omitido (sin columna fecha): datos_eventos\\catastro.xlsx\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/HKLGZ8\n",
      "===================================\n",
      "\n",
      " Archivo: catastro.csv, id=625\n",
      "Guardado en datos_eventos\\catastro.csv\n",
      " Archivo: catastro.geojson, id=624\n",
      "   → No tabular, omitido.\n",
      " Archivo: catastro.kml, id=626\n",
      "   → No tabular, omitido.\n",
      " Archivo: catastro.xlsx, id=623\n",
      "Guardado en datos_eventos\\catastro.xlsx\n",
      " Archivo: catastro.zip, id=627\n",
      "   → No tabular, omitido.\n",
      "Omitido (sin columna fecha): datos_eventos\\catastro.csv\n",
      "Omitido (sin columna fecha): datos_eventos\\catastro.xlsx\n",
      "\n",
      "===================================\n",
      "Procesando dataset: doi:10.71578/TLSNSV\n",
      "===================================\n",
      "\n",
      " Archivo: - Archivo Indice.csv, id=776\n",
      "Guardado en datos_eventos\\- Archivo Indice.csv\n",
      " Archivo: gorum2014.csv, id=774\n",
      "Guardado en datos_eventos\\gorum2014.csv\n",
      " Archivo: gorum2014.geojson, id=773\n",
      "   → No tabular, omitido.\n",
      " Archivo: gorum2014.mat, id=770\n",
      "   → No tabular, omitido.\n",
      " Archivo: gorum2014.npz, id=771\n",
      "   → No tabular, omitido.\n",
      " Archivo: gorum2014.xlsx, id=772\n",
      "Guardado en datos_eventos\\gorum2014.xlsx\n",
      " Archivo: gorum2014.zip, id=775\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.csv, id=768\n",
      "Guardado en datos_eventos\\sepulveda2010.csv\n",
      " Archivo: sepulveda2010.geojson, id=767\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.mat, id=764\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.npz, id=765\n",
      "   → No tabular, omitido.\n",
      " Archivo: sepulveda2010.xlsx, id=766\n",
      "Guardado en datos_eventos\\sepulveda2010.xlsx\n",
      " Archivo: sepulveda2010.zip, id=769\n",
      "   → No tabular, omitido.\n",
      "→ Separando columna fusionada: PUBLICATION|LANDSLIDES|EVENT_DATE|COMMENTS|REFERENCES\n",
      "Omitido (sin columna región): datos_eventos\\- Archivo Indice.csv\n",
      "Omitido (sin columna región): datos_eventos\\gorum2014.csv\n",
      "Omitido (sin columna región): datos_eventos\\gorum2014.xlsx\n",
      "Omitido (sin columna región): datos_eventos\\sepulveda2010.csv\n",
      "Omitido (sin columna región): datos_eventos\\sepulveda2010.xlsx\n",
      "\n",
      "===================================\n",
      "ARCHIVO FINAL GENERADO\n",
      "===================================\n",
      "Dimensiones finales: (46297, 2)\n",
      "   Region       Fecha\n",
      "0  BioBio  2015-12-31\n",
      "1  BioBio  2015-12-31\n",
      "2  BioBio  2016-02-23\n",
      "3  BioBio  2016-02-23\n",
      "4  BioBio  2016-02-28\n",
      "\n",
      "Guardado en: datos_eventos\\eventos_unificados.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyDataverse.api import NativeApi, DataAccessApi\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURACIÓN\n",
    "# ==========================================\n",
    "\n",
    "API_KEY = \"7386f7e6-18cc-49ee-a8eb-d08ac888783d\"\n",
    "BASE_URL = \"https://datospararesiliencia.cl\"\n",
    "\n",
    "DATA_DIR = \"datos_eventos\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "DOIS = [\n",
    "    \"doi:10.71578/FMGXND\",\n",
    "    \"doi:10.71578/UXAUN5\",\n",
    "    \"doi:10.71578/QRDY49\",\n",
    "    \"doi:10.71578/HKLGZ8\",\n",
    "    \"doi:10.71578/TLSNSV\",\n",
    "]\n",
    "\n",
    "VALID_EXT = {\".csv\", \".xlsx\", \".xls\"}\n",
    "\n",
    "\n",
    "api = NativeApi(BASE_URL, API_KEY)\n",
    "data_api = DataAccessApi(BASE_URL, API_KEY)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# UTILIDADES\n",
    "# ==========================================\n",
    "\n",
    "# --- 1: corregir caracteres maldecodificados ---\n",
    "def fix_encoding(col):\n",
    "    try:\n",
    "        return col.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return col\n",
    "\n",
    "\n",
    "# Detectar columna Región\n",
    "def detectar_columna_region(df):\n",
    "    for col in df.columns:\n",
    "        c = col.lower().strip()\n",
    "        if \"regi\" in c:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# Detectar columna Fecha\n",
    "def detectar_columna_fecha(df):\n",
    "    posibles = [\"fecha\", \"fecha_update\"]\n",
    "    for col in df.columns:\n",
    "        if col.lower().strip() in posibles:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# Normalizar nombre de región final\n",
    "def normalizar_region(valor):\n",
    "    v = str(valor).lower()\n",
    "    if \"bio\" in v:\n",
    "        return \"BioBio\"\n",
    "    if \"nuble\" in v:\n",
    "        return \"Ñuble\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# API: descargar archivos\n",
    "# ==========================================\n",
    "def download_dataset_files(doi):\n",
    "    print(\"\\n===================================\")\n",
    "    print(\"Procesando dataset:\", doi)\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    ds = api.get_dataset(doi)\n",
    "\n",
    "    if ds.status_code != 200:\n",
    "        print(\"Error obteniendo dataset:\", doi)\n",
    "        return []\n",
    "\n",
    "    files = ds.json()[\"data\"][\"latestVersion\"][\"files\"]\n",
    "    downloaded_paths = []\n",
    "\n",
    "    for f in files:\n",
    "        fname = f[\"dataFile\"][\"filename\"]\n",
    "        fid = f[\"dataFile\"][\"id\"]\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "\n",
    "        print(f\" Archivo: {fname}, id={fid}\")\n",
    "\n",
    "        if ext not in VALID_EXT:\n",
    "            print(\"   → No tabular, omitido.\")\n",
    "            continue\n",
    "\n",
    "        resp = data_api.get_datafile(fid, is_pid=False)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"Error descargando\", fname)\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(DATA_DIR, fname)\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "\n",
    "        print(\"Guardado en\", path)\n",
    "        downloaded_paths.append(path)\n",
    "\n",
    "    return downloaded_paths\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PROCESAR Y LIMPIAR TABLAS\n",
    "# ==========================================\n",
    "def load_and_clean_file(path):\n",
    "    try:\n",
    "        # -------------------------\n",
    "        # 1. Cargar archivo\n",
    "        # -------------------------\n",
    "        if path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "        elif path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "            df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. Corregir encoding de columnas\n",
    "        # -------------------------\n",
    "        df.columns = [fix_encoding(c) for c in df.columns]\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. Separar columnas fusionadas con \"|\"\n",
    "        # -------------------------\n",
    "        cols_to_split = [c for c in df.columns if \"|\" in c and df[c].notna().sum() > 0]\n",
    "        split_frames = []\n",
    "\n",
    "        for c in cols_to_split:\n",
    "            print(f\"→ Separando columna fusionada: {c}\")\n",
    "            new_cols = c.split(\"|\")\n",
    "            expanded = df[c].astype(str).str.split(\"|\", expand=True)\n",
    "\n",
    "            if expanded.shape[1] == len(new_cols):\n",
    "                expanded.columns = new_cols\n",
    "                split_frames.append(expanded)\n",
    "                df = df.drop(columns=[c])\n",
    "\n",
    "        for f in split_frames:\n",
    "            df = pd.concat([df, f], axis=1)\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. Detectar columnas útiles\n",
    "        # -------------------------\n",
    "        col_region = detectar_columna_region(df)\n",
    "        if col_region is None:\n",
    "            print(f\"Omitido (sin columna región): {path}\")\n",
    "            return None\n",
    "\n",
    "        col_fecha = detectar_columna_fecha(df)\n",
    "        if col_fecha is None:\n",
    "            print(f\"Omitido (sin columna fecha): {path}\")\n",
    "            return None\n",
    "\n",
    "        # -------------------------\n",
    "        # 5. Normalizar región\n",
    "        # -------------------------\n",
    "        df[col_region] = (\n",
    "            df[col_region]\n",
    "            .astype(str)\n",
    "            .apply(fix_encoding)      # corregir caracteres maldecodificados\n",
    "            .str.normalize(\"NFKD\")    # normalizar tildes\n",
    "            .str.encode(\"ascii\", \"ignore\")  # convertir Ñ -> n si fuera necesario\n",
    "            .str.decode(\"utf8\")\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # 6. Filtrar solo regiones relevantes\n",
    "        # -------------------------\n",
    "        mask = df[col_region].str.contains(\"bio\") | df[col_region].str.contains(\"nuble\")\n",
    "        df = df[mask]\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"   ⚠ Archivo sin filas de Biobío/Ñuble: {path}\")\n",
    "            return None\n",
    "\n",
    "        # -------------------------\n",
    "        # 7. Convertir la fecha a formato YYYY-MM-DD\n",
    "        # -------------------------\n",
    "        df[col_fecha] = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n",
    "\n",
    "        # eliminar filas sin fecha válida\n",
    "        df = df.dropna(subset=[col_fecha])\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"   ⚠ Archivo sin fechas válidas después de normalizar: {path}\")\n",
    "            return None\n",
    "\n",
    "        df[col_fecha] = df[col_fecha].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 8. Normalizar nombres finales de región\n",
    "        # -------------------------\n",
    "        df[\"Region\"] = df[col_region].apply(normalizar_region)\n",
    "        df[\"Fecha\"] = df[col_fecha]\n",
    "\n",
    "        df = df[[\"Region\", \"Fecha\"]]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error leyendo/limpiando\", path, e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PROCESAR TODOS LOS DATASETS\n",
    "# ==========================================\n",
    "\n",
    "all_clean_dfs = []\n",
    "\n",
    "for doi in DOIS:\n",
    "    paths = download_dataset_files(doi)\n",
    "\n",
    "    for path in paths:\n",
    "        df_clean = load_and_clean_file(path)\n",
    "        if df_clean is not None:\n",
    "            all_clean_dfs.append(df_clean)\n",
    "\n",
    "if not all_clean_dfs:\n",
    "    raise RuntimeError(\"No se pudo cargar ningún archivo válido con región y fecha.\")\n",
    "\n",
    "# ==========================================\n",
    "# UNIFICAR\n",
    "# ==========================================\n",
    "\n",
    "df_final = pd.concat(all_clean_dfs, ignore_index=True)\n",
    "\n",
    "output_path = os.path.join(DATA_DIR, \"eventos_unificados.csv\")\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n===================================\")\n",
    "print(\"ARCHIVO FINAL GENERADO\")\n",
    "print(\"===================================\")\n",
    "print(\"Dimensiones finales:\", df_final.shape)\n",
    "print(df_final.head())\n",
    "print(f\"\\nGuardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d24e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna Región detectada como: REGIÓN\n",
      "Columna Fecha detectada como: Fecha\n",
      "Columna Inversión detectada como: INVERSIÓN (MILES DE $ DE CADA AÑO)\n",
      "\n",
      "Archivo generado correctamente: inversion_filtrada.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "def fix_encoding(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Cargar archivo\n",
    "# -----------------------------------------\n",
    "input_file = \"inversion_semestres.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. Intentar detectar columna Región\n",
    "# -----------------------------------------\n",
    "def normalize_for_match(s):\n",
    "    \"\"\"Normaliza texto eliminando tildes y bajando a minúsculas.\"\"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.lower().strip()\n",
    "\n",
    "normalized_cols = {normalize_for_match(c): c for c in df.columns}\n",
    "\n",
    "possible_region_keys = [\n",
    "    \"region\",\n",
    "    \"región\",\n",
    "    \"region administrativa\",\n",
    "    \"nombre region\",\n",
    "    \"nombre región\",\n",
    "]\n",
    "\n",
    "col_region = None\n",
    "for key in possible_region_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_region = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_region is None:\n",
    "    print(\"\\nNo se encontró la columna Región. Columnas disponibles:\")\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "    raise KeyError(\"No existe columna equivalente a 'Región' en el archivo.\")\n",
    "\n",
    "print(\"Columna Región detectada como:\", col_region)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Detectar columna Fecha\n",
    "# -----------------------------------------\n",
    "possible_fecha_keys = [\"fecha\", \"date\", \"fechas\"]\n",
    "\n",
    "col_fecha = None\n",
    "for key in possible_fecha_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_fecha = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_fecha is None:\n",
    "    raise KeyError(\"No se encontró una columna de fecha.\")\n",
    "\n",
    "print(\"Columna Fecha detectada como:\", col_fecha)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Detectar columna Inversión\n",
    "# -----------------------------------------\n",
    "possible_inv_keys = [\n",
    "    \"inversion\",\n",
    "    \"inversión\",\n",
    "    \"inversion (miles de $ de cada año)\",\n",
    "    \"monto inversion\",\n",
    "    \"monto\",\n",
    "]\n",
    "\n",
    "col_inv = None\n",
    "for key in possible_inv_keys:\n",
    "    key_norm = normalize_for_match(key)\n",
    "    if key_norm in normalized_cols:\n",
    "        col_inv = normalized_cols[key_norm]\n",
    "        break\n",
    "\n",
    "if col_inv is None:\n",
    "    print(\"\\nNo se encontró la columna de inversión. Columnas disponibles:\")\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "    raise KeyError(\"No existe columna de inversión en el archivo.\")\n",
    "\n",
    "print(\"Columna Inversión detectada como:\", col_inv)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5. Normalizar Región\n",
    "# -----------------------------------------\n",
    "df[col_region] = (\n",
    "    df[col_region]\n",
    "    .astype(str)\n",
    "    .apply(fix_encoding)\n",
    "    .str.normalize(\"NFKD\")\n",
    "    .str.encode(\"ascii\", \"ignore\")\n",
    "    .str.decode(\"utf8\")\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Variantes aceptadas\n",
    "bio_vars = [\"bio bio\", \"biobio\", \"bio-bio\"]\n",
    "nuble_vars = [\"nuble\", \"ñuble\"]\n",
    "\n",
    "df_filtered = df[df[col_region].isin(bio_vars + nuble_vars)].copy()\n",
    "\n",
    "# Mapear al nombre final\n",
    "def map_region(r):\n",
    "    if r in bio_vars:\n",
    "        return \"BioBio\"\n",
    "    if r in nuble_vars:\n",
    "        return \"Ñuble\"\n",
    "    return None\n",
    "\n",
    "df_filtered[\"Region\"] = df_filtered[col_region].apply(map_region)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Normalizar fecha\n",
    "# -----------------------------------------\n",
    "df_filtered[\"Fecha\"] = pd.to_datetime(df_filtered[col_fecha], dayfirst=True, errors=\"coerce\").dt.date\n",
    "\n",
    "# -----------------------------------------\n",
    "# Normalizar inversión\n",
    "# -----------------------------------------\n",
    "df_filtered[\"Inversion\"] = (\n",
    "    df_filtered[col_inv]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[^0-9,.\\-]\", \"\", regex=True)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .astype(float)\n",
    "    .mul(1000)\n",
    "    .round(0)        # redondea al entero más cercano\n",
    "    .astype(int)     # convierte a entero\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Exportar CSV\n",
    "# -----------------------------------------\n",
    "output_file = \"inversion_filtrada.csv\"\n",
    "df_filtered[[\"Region\", \"Fecha\", \"Inversion\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\nArchivo generado correctamente:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ecf66e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region: BioBio | Observaciones semestrales: 42 | Train: 33 | Test: 9\n",
      "\n",
      "=== Resultados para BioBio ===\n",
      "Order (p,d,q): (0, 1, 2)\n",
      "Métricas (test): {'MAE': 14064731957.649183, 'RMSE': np.float64(18059359118.531216), 'MAPE': np.float64(32.025112399108245)}\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:              Inversion   No. Observations:                   33\n",
      "Model:               SARIMAX(0, 1, 2)   Log Likelihood                -722.612\n",
      "Date:                Mon, 08 Dec 2025   AIC                           1453.223\n",
      "Time:                        20:55:12   BIC                           1458.692\n",
      "Sample:                    01-01-2000   HQIC                          1454.936\n",
      "                         - 01-01-2016                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "Forecast guardado en forecast_BioBio.csv\n",
      "Gráfico guardado en forecast_BioBio.png\n",
      "\n",
      "Region: Ñuble | Observaciones semestrales: 6 | Train: 4 | Test: 2\n",
      "\n",
      "=== Resultados para Ñuble ===\n",
      "Order (p,d,q): (0, 1, 2)\n",
      "Métricas (test): {'MAE': 2825341525.515465, 'RMSE': np.float64(3466690439.362078), 'MAPE': np.float64(15.85587118966782)}\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:              Inversion   No. Observations:                    4\n",
      "Model:               SARIMAX(0, 1, 2)   Log Likelihood                   0.000\n",
      "Date:                Mon, 08 Dec 2025   AIC                              8.000\n",
      "Time:                        20:55:12   BIC                               -inf\n",
      "Sample:                    01-01-2018   HQIC                               nan\n",
      "                         - 07-01-2019                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "Forecast guardado en forecast_Ñuble.csv\n",
      "Gráfico guardado en forecast_Ñuble.png\n",
      "\n",
      "Proceso finalizado. Revisa los archivos forecast_*.csv y forecast_*.png.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ARIMAX forecasting (SARIMAX) para regiones BioBio y Ñuble.\n",
    "Entrada:\n",
    " - eventos_unificados.csv  (columns: Region, Fecha)  Fecha puede ser yyyy-mm-dd o similar\n",
    " - inversion_filtrada.csv  (columns: Region, Fecha, Inversion)  Fecha = YYYY-01-01 o YYYY-07-01 (semestres)\n",
    "Salida:\n",
    " - Prints métricas y resumen por región\n",
    " - Guarda gráficos: forecast_BioBio.png y forecast_Ñuble.png (si hay datos)\n",
    " - Guarda CSVs de forecast: forecast_BioBio.csv, forecast_Ñuble.csv\n",
    "Notas:\n",
    " - Forecast: 10 años = 20 semestres\n",
    " - División train/test: 80% train (temporal)\n",
    " - Selección de orden (p,d,q) por búsqueda de AIC con p,q ∈ {0,1,2}, d fijo = 1 (puedes cambiar)\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "EVENTS_FILE = \"datos_eventos/eventos_unificados.csv\"\n",
    "INV_FILE = \"inversion_filtrada.csv\"\n",
    "REGIONS = [\"BioBio\", \"Ñuble\"]  # regiones a procesar\n",
    "FORECAST_YEARS = 10\n",
    "SEMS_PER_YEAR = 2\n",
    "FORECAST_PERIODS = FORECAST_YEARS * SEMS_PER_YEAR  # 20 semestres\n",
    "TRAIN_FRAC = 0.8\n",
    "MAX_P = 2\n",
    "MAX_Q = 2\n",
    "D = 1  # grado de differencing\n",
    "\n",
    "# -------------------------\n",
    "# Utilidades\n",
    "# -------------------------\n",
    "def normalize_region_name(s):\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    s = str(s).strip()\n",
    "    low = s.lower()\n",
    "    if \"bio\" in low:\n",
    "        return \"BioBio\"\n",
    "    if \"ñuble\" in low or \"nuble\" in low:\n",
    "        return \"Ñuble\"\n",
    "    return s\n",
    "\n",
    "def to_semester_start(ts):\n",
    "    # ts: datetime-like\n",
    "    ts = pd.to_datetime(ts, errors='coerce')\n",
    "    if pd.isna(ts):\n",
    "        return pd.NaT\n",
    "    if ts.month <= 6:\n",
    "        return pd.Timestamp(year=ts.year, month=1, day=1)\n",
    "    else:\n",
    "        return pd.Timestamp(year=ts.year, month=7, day=1)\n",
    "\n",
    "def load_data():\n",
    "    if not os.path.exists(EVENTS_FILE):\n",
    "        raise FileNotFoundError(f\"No encuentro {EVENTS_FILE} en el directorio actual.\")\n",
    "    if not os.path.exists(INV_FILE):\n",
    "        raise FileNotFoundError(f\"No encuentro {INV_FILE} en el directorio actual.\")\n",
    "    eventos = pd.read_csv(EVENTS_FILE)\n",
    "    inversion = pd.read_csv(INV_FILE)\n",
    "    return eventos, inversion\n",
    "\n",
    "def prepare(eventos, inversion):\n",
    "    # Normalizar nombres\n",
    "    eventos['Region'] = eventos['Region'].apply(normalize_region_name)\n",
    "    inversion['Region'] = inversion['Region'].apply(normalize_region_name)\n",
    "    # Parse fecha\n",
    "    eventos['Fecha'] = pd.to_datetime(eventos['Fecha'], errors='coerce')\n",
    "    inversion['Fecha'] = pd.to_datetime(inversion['Fecha'], errors='coerce')\n",
    "    # Mapear eventos a semestre (semester start)\n",
    "    eventos['Semestre'] = eventos['Fecha'].apply(to_semester_start)\n",
    "    # Contar eventos por region y semestre\n",
    "    ev_counts = (eventos\n",
    "                 .dropna(subset=['Semestre'])\n",
    "                 .groupby(['Region','Semestre'])\n",
    "                 .size()\n",
    "                 .reset_index(name='Eventos'))\n",
    "    ev_counts = ev_counts.rename(columns={'Semestre':'Fecha'})\n",
    "    # Asegurar que inversion tenga semestres normalizados (redondear al inicio de semestre)\n",
    "    inversion['Fecha'] = inversion['Fecha'].apply(to_semester_start)\n",
    "    # Agregar inversion por region+fecha (si hay duplicados)\n",
    "    inv_agg = inversion.groupby(['Region','Fecha'], as_index=False)['Inversion'].sum()\n",
    "    # Merge outer para que entren semestres faltantes\n",
    "    merged = pd.merge(inv_agg, ev_counts, on=['Region','Fecha'], how='outer')\n",
    "    merged['Inversion'] = merged['Inversion'].fillna(0.0)\n",
    "    merged['Eventos'] = merged['Eventos'].fillna(0).astype(int)\n",
    "    merged = merged.sort_values(['Region','Fecha']).reset_index(drop=True)\n",
    "    return merged\n",
    "\n",
    "def ensure_semester_index(df_region):\n",
    "    # df_region: tiene columnas Fecha, Inversion, Eventos para una region\n",
    "    df_region = df_region.set_index('Fecha').sort_index()\n",
    "    # crear rango completo desde primer a ultimo semestre con freq '6MS' anclado en inicio de semestre\n",
    "    start = df_region.index.min()\n",
    "    end = df_region.index.max()\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        return pd.DataFrame(columns=['Inversion','Eventos'])\n",
    "    idx = pd.date_range(start=start, end=end, freq='6MS')  # cada semestre\n",
    "    s = df_region.reindex(idx)\n",
    "    s.index.name = 'Fecha'\n",
    "    s['Inversion'] = s['Inversion'].fillna(0.0)\n",
    "    s['Eventos'] = s['Eventos'].fillna(0).astype(int)\n",
    "    return s\n",
    "\n",
    "def select_order_by_aic(endog, exog, max_p=2, max_q=2, d=1):\n",
    "    best_aic = np.inf\n",
    "    best_order = (1,d,1)\n",
    "    for p in range(0, max_p+1):\n",
    "        for q in range(0, max_q+1):\n",
    "            try:\n",
    "                model = SARIMAX(endog, exog=exog, order=(p,d,q),\n",
    "                                enforce_stationarity=False, enforce_invertibility=False)\n",
    "                res = model.fit(disp=False, maxiter=200)\n",
    "                if res.aic < best_aic:\n",
    "                    best_aic = res.aic\n",
    "                    best_order = (p,d,q)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return best_order\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denom = np.where(np.abs(y_true) < 1e-9, 1.0, np.abs(y_true))\n",
    "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline por región\n",
    "# -------------------------\n",
    "def run_for_region(merged, region, forecast_periods=FORECAST_PERIODS):\n",
    "    df_reg = merged[merged['Region'] == region][['Fecha','Inversion','Eventos']].copy()\n",
    "    if df_reg.empty:\n",
    "        print(f\"[WARN] No hay datos para la región {region}. Omitiendo.\")\n",
    "        return None\n",
    "    df_reg['Fecha'] = pd.to_datetime(df_reg['Fecha'])\n",
    "    series = ensure_semester_index(df_reg)\n",
    "    if series.empty:\n",
    "        print(f\"[WARN] Region {region} tiene índice vacío después de resample semestral.\")\n",
    "        return None\n",
    "\n",
    "    # División train/test temporal (80% train)\n",
    "    n = len(series)\n",
    "    train_n = int(np.floor(n * TRAIN_FRAC))\n",
    "    if train_n < 4:\n",
    "        train_n = max(1, n - 2)\n",
    "    train_idx = series.index[:train_n]\n",
    "    test_idx = series.index[train_n:]\n",
    "\n",
    "    endog = series['Inversion']\n",
    "    exog = series[['Eventos']]\n",
    "\n",
    "    # Selección de orden por AIC\n",
    "    print(f\"\\nRegion: {region} | Observaciones semestrales: {n} | Train: {train_n} | Test: {n-train_n}\")\n",
    "    try:\n",
    "        order = select_order_by_aic(endog.loc[train_idx], exog.loc[train_idx], max_p=MAX_P, max_q=MAX_Q, d=D)\n",
    "    except Exception as e:\n",
    "        print(\"  [WARN] Error en selección orden por AIC, usando (1,1,1). Error:\", e)\n",
    "        order = (1,D,1)\n",
    "\n",
    "    # Ajuste modelo en train\n",
    "    model = SARIMAX(endog.loc[train_idx], exog=exog.loc[train_idx], order=order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False)\n",
    "    res = model.fit(disp=False, maxiter=200)\n",
    "\n",
    "    # Predicción en test (si existe test)\n",
    "    metrics = {}\n",
    "    if len(test_idx) > 0:\n",
    "        pred_test_res = res.get_prediction(start=test_idx[0], end=test_idx[-1], exog=exog.loc[test_idx], dynamic=False)\n",
    "        pred_test = pred_test_res.predicted_mean\n",
    "        true_test = endog.loc[test_idx]\n",
    "        mae = mean_absolute_error(true_test, pred_test)\n",
    "        rmse = np.sqrt(mean_squared_error(true_test, pred_test))\n",
    "        mape_val = mape(true_test, pred_test)\n",
    "        metrics = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape_val}\n",
    "    else:\n",
    "        pred_test = pd.Series(dtype=float)\n",
    "        metrics = {'MAE': None, 'RMSE': None, 'MAPE': None}\n",
    "\n",
    "    # Forecast futuro: necesitamos exog futuros. Estrategia simple: usar media de últimos 4 exog observados.\n",
    "    last_exog = exog.iloc[-4:]['Eventos'].values\n",
    "    if len(last_exog) == 0:\n",
    "        future_exog_vals = np.zeros(forecast_periods)\n",
    "    else:\n",
    "        fill_val = float(np.mean(last_exog))\n",
    "        future_exog_vals = np.full(forecast_periods, fill_val)\n",
    "    # Índice para futuros semestres\n",
    "    start_future = series.index[-1] + pd.DateOffset(months=6)\n",
    "    future_index = pd.date_range(start=start_future, periods=forecast_periods, freq='6MS')\n",
    "\n",
    "    # Forecast con get_forecast\n",
    "    forecast_res = res.get_forecast(steps=forecast_periods, exog=future_exog_vals.reshape(-1,1))\n",
    "    forecast_mean = forecast_res.predicted_mean\n",
    "    forecast_ci = forecast_res.conf_int()\n",
    "\n",
    "    # Construir dataframes de salida\n",
    "    df_forecast = pd.DataFrame({\n",
    "        'Fecha': future_index,\n",
    "        'Forecast': forecast_mean.values,\n",
    "        'Lower_CI': forecast_ci.iloc[:,0].values,\n",
    "        'Upper_CI': forecast_ci.iloc[:,1].values,\n",
    "        'Exog_Pred_Eventos': future_exog_vals\n",
    "    }).set_index('Fecha')\n",
    "\n",
    "    # Guardar resultados\n",
    "    out = {\n",
    "        'region': region,\n",
    "        'order': order,\n",
    "        'model_result': res,\n",
    "        'train_index': train_idx,\n",
    "        'test_index': test_idx,\n",
    "        'pred_test': pred_test,\n",
    "        'true_test': endog.loc[test_idx],\n",
    "        'metrics': metrics,\n",
    "        'forecast_df': df_forecast,\n",
    "        'series': series\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    try:\n",
    "        eventos, inversion = load_data()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR al cargar archivos:\", e)\n",
    "        return\n",
    "\n",
    "    merged = prepare(eventos, inversion)\n",
    "    # Asegurar que Fecha sea datetime\n",
    "    merged['Fecha'] = pd.to_datetime(merged['Fecha'])\n",
    "    # Procesar cada región\n",
    "    results = {}\n",
    "    for region in REGIONS:\n",
    "        res = run_for_region(merged, region)\n",
    "        if res is None:\n",
    "            continue\n",
    "        results[region] = res\n",
    "\n",
    "        # Imprimir métricas\n",
    "        print(f\"\\n=== Resultados para {region} ===\")\n",
    "        print(\"Order (p,d,q):\", res['order'])\n",
    "        print(\"Métricas (test):\", res['metrics'])\n",
    "        # resumen del modelo (primeras tablas)\n",
    "        try:\n",
    "            print(res['model_result'].summary().tables[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Guardar forecast a CSV\n",
    "        fname = f\"forecast_{region}.csv\"\n",
    "        res['forecast_df'].to_csv(fname, index=True)\n",
    "        print(f\"Forecast guardado en {fname}\")\n",
    "\n",
    "        # Graficar observed, test pred, forecast\n",
    "        plt.figure(figsize=(12,6))\n",
    "        series = res['series']\n",
    "        plt.plot(series.index, series['Inversion'], label='Observed (Inversion)', marker='o')\n",
    "        if len(res['test_index'])>0 and not res['pred_test'].empty:\n",
    "            plt.plot(res['pred_test'].index, res['pred_test'].values, label='Predicted (test)', linestyle='--', marker='x')\n",
    "        plt.plot(res['forecast_df'].index, res['forecast_df']['Forecast'], label=f'Forecast ({FORECAST_YEARS} yrs)', marker='s')\n",
    "        plt.fill_between(res['forecast_df'].index, res['forecast_df']['Lower_CI'], res['forecast_df']['Upper_CI'], alpha=0.25)\n",
    "        plt.title(f\"ARIMAX Forecast - {region}\")\n",
    "        plt.xlabel(\"Fecha (semestre)\")\n",
    "        plt.ylabel(\"Inversión\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        imgname = f\"forecast_{region}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(imgname)\n",
    "        plt.close()\n",
    "        print(f\"Gráfico guardado en {imgname}\")\n",
    "\n",
    "    print(\"\\nProceso finalizado. Revisa los archivos forecast_*.csv y forecast_*.png.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
